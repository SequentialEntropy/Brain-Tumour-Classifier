{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment and Split into Training/Validation and Testing\n",
    "\n",
    "Only the training dataset will be augmented.\n",
    "\n",
    "The testing dataset is a holdout dataset, hence, will **not** be used to train or validate models, so that it is never exposed to the models during training.\n",
    "\n",
    "It is purely for use at the end when comparing the various models' responses to new unseen data.\n",
    "\n",
    "## This program will do the following steps in order:\n",
    "\n",
    "1. Load the entire `brain_tumor_dataset`\n",
    "\n",
    "2. Split the dataset into `training_and_validation_dataset` and `testing_dataset`\n",
    "\n",
    "3. Augment only the `training_and_validation_dataset` to make extra copies with random transformations applied\n",
    "\n",
    "4. Save the `training_and_validation_dataset` and `testing_dataset` into its respective directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.utils import image_dataset_from_directory, split_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomContrast\n",
    "from tensorflow.image import encode_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constants and Parameters\n",
    "\"\"\"\n",
    "IMAGE_SIZE = (150, 150)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "AUGMENTATION_COPIES = 3 # How many augmented copies per image\n",
    "MAX_ROTATION = 0.0277   # 0.0277 radians = 5 degrees\n",
    "MAX_ZOOM = 0.05         # Small enough to prevent stretches in one direction but not the other\n",
    "MAX_CONTRAST = 0.2\n",
    "\n",
    "INPUT_DIRECTORY = \"brain_tumor_dataset\"\n",
    "TRAINING_DIRECTORY = \"training_and_validation_dataset\" # (Augmented)\n",
    "TESTING_DIRECTORY = \"testing_dataset\" # (Not augmented) NEVER used in training - only used in evaluation\n",
    "\n",
    "TRAINING_SPLIT = 0.8 # 80% train & validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 223 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load images\n",
    "\"\"\"\n",
    "dataset = image_dataset_from_directory(\n",
    "    INPUT_DIRECTORY,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into the training and testing portions\n",
    "\"\"\"\n",
    "training_dataset, testing_dataset = split_dataset(dataset, left_size=TRAINING_SPLIT, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset count before split:    223\n",
      "Training & Validation dataset count: 178\n",
      "Testing dataset count:               45\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Count the number of images in each dataset\n",
    "\"\"\"\n",
    "def count_images(dataset):\n",
    "    total_images = 0\n",
    "    for batch, _ in dataset:\n",
    "        for _ in batch:\n",
    "            total_images += 1\n",
    "    return total_images\n",
    "\n",
    "print(f\"Total dataset count before split:    {count_images(dataset)}\")\n",
    "print(f\"Training & Validation dataset count: {count_images(training_dataset)}\")\n",
    "print(f\"Testing dataset count:               {count_images(testing_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define augmentation function\n",
    "\"\"\"\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(MAX_ROTATION, fill_mode=\"nearest\"),\n",
    "    # Only change the width to prevent dramatic stretches\n",
    "    RandomZoom(height_factor=(0, 0), width_factor=(-1 * MAX_ZOOM, MAX_ZOOM), fill_mode=\"nearest\"),\n",
    "    RandomContrast(MAX_CONTRAST),\n",
    "])\n",
    "\n",
    "def augment_image(image, label):\n",
    "    return data_augmentation(image, training=True), label\n",
    "\n",
    "def augment_dataset(dataset):\n",
    "    # Add extra augmented images to original training dataset\n",
    "    augmented_datasets = [dataset.map(augment_image) for _ in range(AUGMENTATION_COPIES)]\n",
    "\n",
    "    # Start with the original dataset\n",
    "    total_dataset = dataset\n",
    "\n",
    "    # Concatenate augmented datasets sequentially\n",
    "    for dataset in augmented_datasets:\n",
    "        total_dataset = total_dataset.concatenate(dataset)\n",
    "\n",
    "    return total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augment images\n",
    "\"\"\"\n",
    "augmented_training_dataset = augment_dataset(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation copies per image:                           3\n",
      "Training & Validation dataset count before augmentation: 178\n",
      "Training & Validation dataset count after augmentation:  712\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Count images after augmentation\n",
    "\"\"\"\n",
    "print(f\"Augmentation copies per image:                           {AUGMENTATION_COPIES}\")\n",
    "print(f\"Training & Validation dataset count before augmentation: {count_images(training_dataset)}\")\n",
    "print(f\"Training & Validation dataset count after augmentation:  {count_images(augmented_training_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 712 images to training_and_validation_dataset ({'yes': 484, 'no': 228})\n",
      "Saved 45 images to testing_dataset ({'yes': 29, 'no': 16})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save dataset\n",
    "\"\"\"\n",
    "def save_dataset(dataset, directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    total_images = 0\n",
    "    class_count = {\n",
    "        \"yes\": 0,\n",
    "        \"no\": 0,\n",
    "    }\n",
    "    for batch, labels in dataset:\n",
    "        for image, label in zip(batch, labels):\n",
    "            image = tf.cast(image, tf.uint8)  # Convert for saving\n",
    "\n",
    "            output_filename = f\"{total_images:05d}.png\"\n",
    "\n",
    "            class_name = \"yes\" if int(label.numpy()) == 1 else \"no\"\n",
    "            output_path = os.path.join(directory, class_name, output_filename)\n",
    "\n",
    "            tf.io.write_file(output_path, encode_png(image))\n",
    "\n",
    "            total_images += 1\n",
    "            class_count[class_name] += 1\n",
    "\n",
    "    print(f\"Saved {total_images} images to {directory} ({class_count})\")\n",
    "\n",
    "save_dataset(augmented_training_dataset, TRAINING_DIRECTORY)\n",
    "save_dataset(testing_dataset, TESTING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
